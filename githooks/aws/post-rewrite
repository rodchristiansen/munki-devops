#!/usr/bin/env bash
#
# ──────────────────────────────────────────────────────────────────────────────
#  post-rewrite hook  –  targeted package downloads after rebase/amend (AWS S3)
#
#  This hook fires after git operations that rewrite commits:
#       - `git rebase`
#       - `git commit --amend`
#       - `git filter-branch`
#
#  When commits are rewritten, package references in pkgsinfo may change.
#  This hook ensures your local pkgs directory stays in sync with S3.
#
#  Default behavior (no flags):
#       - Parses changed pkgsinfo files (YAML/plist)
#       - Extracts installer_item_location from each
#       - Downloads ONLY those specific packages from S3
#       - Syncs catalogs/icons folders if changed
#
#  Flags:
#       --sync    : Full folder sync (incremental)
#       --force   : Full download with confirmation (forces re-download of all)
#       --dry-run : Show what would be downloaded without actually downloading
#       --path    : Download a specific package path
#
#  Configuration (set these environment variables or edit defaults below):
#       MUNKI_REPO_PATH  - Path to your Munki repo (default: ~/munki-repo)
#       MUNKI_S3_BUCKET  - S3 bucket name (e.g., your-munki-bucket)
#       MUNKI_S3_PREFIX  - Optional prefix/folder in bucket (default: empty)
#       AWS_REGION       - AWS region (default: us-east-1)
#       AWS_PROFILE      - Optional AWS CLI profile to use
#
#  Daily rotating log kept in .git/logs (7-day retention).
# ──────────────────────────────────────────────────────────────────────────────

set -euo pipefail
set -f
shopt -s nullglob

# ── Configuration ──────────────────────────────────────────
BASE="${MUNKI_REPO_PATH:-$HOME/munki-repo}"
S3_BUCKET="${MUNKI_S3_BUCKET:-your-munki-bucket}"
S3_PREFIX="${MUNKI_S3_PREFIX:-}"
AWS_REGION="${AWS_REGION:-us-east-1}"

# Build S3 URL (with optional prefix)
if [[ -n "$S3_PREFIX" ]]; then
  S3_URL="s3://${S3_BUCKET}/${S3_PREFIX}"
else
  S3_URL="s3://${S3_BUCKET}"
fi

# ── check for AWS CLI ──────────────────────────────────────
if ! command -v aws &>/dev/null; then
  echo "" >&2
  echo "ERROR: AWS CLI not found" >&2
  echo "" >&2
  echo "The post-rewrite hook needs the AWS CLI to download packages from S3." >&2
  echo "" >&2
  echo "To install AWS CLI:" >&2
  echo "  brew install awscli" >&2
  echo "" >&2
  echo "Or download from: https://aws.amazon.com/cli/" >&2
  echo "" >&2
  exit 1
fi

# ── command line options ───────────────────────────────────
SYNC_MODE=false
FORCE_MODE=false
DRY_RUN=false
TARGET_PATHS=()

while [[ $# -gt 0 ]]; do
  case $1 in
    --sync)
      SYNC_MODE=true
      shift
      ;;
    --force)
      FORCE_MODE=true
      shift
      ;;
    --dry-run)
      DRY_RUN=true
      shift
      ;;
    --path)
      shift
      [[ $# -gt 0 ]] && TARGET_PATHS+=("$1")
      shift
      ;;
    *)
      shift
      ;;
  esac
done

# ── logging ────────────────────────────────────────────────
LOGDIR="$BASE/.git/logs"
mkdir -p "$LOGDIR"
TODAY=$(date +%F)
LOGFILE="$LOGDIR/hook-post-rewrite-download-$TODAY.log"
: >"$LOGFILE"
find "$LOGDIR" -name 'hook-post-rewrite-download-*.log' -mtime +7 -exec rm {} \;

log() {
  local ts=$(date '+%Y-%m-%d %H:%M:%S')
  printf '%s %s\n' "$ts" "$*" | tee -a "$LOGFILE"
}

next_index() {
  local max=0
  for f in "$LOGDIR"/hook-post-rewrite-download-"$TODAY"-*; do
    [[ -e $f ]] || continue
    local n=${f##*-}; n=${n%.log}
    (( n > max )) && max=$n
  done
  echo $((max+1))
}

# ── AWS authentication check ───────────────────────────────
check_aws_auth() {
  AWS_CMD=(aws)
  [[ -n "${AWS_PROFILE:-}" ]] && AWS_CMD+=(--profile "$AWS_PROFILE")
  AWS_CMD+=(--region "$AWS_REGION")
  
  if ! "${AWS_CMD[@]}" sts get-caller-identity &>/dev/null; then
    log "AWS CLI not authenticated. Please configure credentials first."
    log "Run: aws configure"
    exit 1
  fi
}

# Build AWS command with profile/region
aws_cmd() {
  local cmd=(aws)
  [[ -n "${AWS_PROFILE:-}" ]] && cmd+=(--profile "$AWS_PROFILE")
  cmd+=(--region "$AWS_REGION")
  "${cmd[@]}" "$@"
}

# ── parse pkgsinfo files for installer locations ───────────
extract_location_from_pkgsinfo() {
  local pkgsinfo_file="$1"
  local location=""
  
  if [[ "$pkgsinfo_file" =~ \.(yaml|yml)$ ]]; then
    location=$(grep -E '^\s*installer_item_location:\s*' "$pkgsinfo_file" 2>/dev/null | sed -E 's/^\s*installer_item_location:\s*//' | tr -d '"' | xargs | head -1)
  else
    location=$(plutil -extract installer_item_location raw "$pkgsinfo_file" 2>/dev/null | xargs || true)
  fi
  
  echo "$location"
}

extract_installer_locations() {
  local changed_pkgsinfo="$1"
  local installer_locations=()
  
  while IFS= read -r pkgsinfo_file; do
    [[ -z "$pkgsinfo_file" ]] && continue
    [[ ! -f "$BASE/$pkgsinfo_file" ]] && continue
    
    local location=$(extract_location_from_pkgsinfo "$BASE/$pkgsinfo_file")
    
    if [[ -n "$location" ]]; then
      installer_locations+=("$location")
      log "  Found package: $location"
    fi
  done <<< "$changed_pkgsinfo"
  
  printf '%s\n' "${installer_locations[@]}"
}

# ── build canonical package list from ALL pkgsinfo files ───
build_canonical_pkg_list() {
  local pkgsinfo_dir="$BASE/pkgsinfo"
  local canonical_list=()
  
  while IFS= read -r -d '' pkgsinfo_file; do
    local location=$(extract_location_from_pkgsinfo "$pkgsinfo_file")
    if [[ -n "$location" ]]; then
      canonical_list+=("$location")
    fi
  done < <(find "$pkgsinfo_dir" -type f \( -name "*.yaml" -o -name "*.yml" -o -name "*.plist" -o ! -name "*.*" \) -print0 2>/dev/null)
  
  printf '%s\n' "${canonical_list[@]}" | sort -u
}

# ── cleanup orphan packages not referenced by any pkgsinfo ─
cleanup_orphan_packages() {
  local pkgs_dir="$BASE/pkgs"
  local canonical_file=$(mktemp)
  local orphan_count=0
  
  log "Building canonical package list from pkgsinfo files..."
  build_canonical_pkg_list > "$canonical_file"
  local canonical_count=$(wc -l < "$canonical_file" | xargs)
  log "Found $canonical_count packages referenced in pkgsinfo"
  
  log "Scanning for orphan packages..."
  while IFS= read -r -d '' pkg_file; do
    local rel_path="${pkg_file#$pkgs_dir/}"
    
    if ! grep -qxF "$rel_path" "$canonical_file"; then
      log "  Orphan: $rel_path"
      if [[ "$DRY_RUN" == "true" ]]; then
        log "    [DRY RUN] Would delete"
      else
        rm -f "$pkg_file"
        log "    Deleted"
      fi
      ((orphan_count++))
    fi
  done < <(find "$pkgs_dir" -type f \( -name "*.pkg" -o -name "*.dmg" -o -name "*.zip" -o -name "*.mobileconfig" \) -print0 2>/dev/null)
  
  rm -f "$canonical_file"
  
  if [[ $orphan_count -gt 0 ]]; then
    log "Cleaned up $orphan_count orphan package(s)"
    find "$pkgs_dir" -type d -empty -delete 2>/dev/null || true
  else
    log "No orphan packages found"
  fi
}

# ── change detection ───────────────────────────────────────
detect_changes() {
  if [[ "$FORCE_MODE" == "true" ]]; then
    echo "force"
    return
  fi
  
  if [[ "$SYNC_MODE" == "true" ]]; then
    echo "sync"
    return
  fi
  
  if ! git rev-parse HEAD~1 >/dev/null 2>&1; then
    log "No previous commit available for change detection. Use --sync for full sync."
    echo "none"
    return
  fi
  
  local changed_files=$(git diff --name-only HEAD~1 HEAD -- pkgsinfo/ catalogs/ icons/ 2>/dev/null || true)
  
  if [[ -z "$changed_files" ]]; then
    log "No changes detected in tracked directories. Skipping sync."
    echo "none"
    return
  fi
  
  local needs_catalogs=false
  local needs_icons=false
  local changed_pkgsinfo=""
  
  while IFS= read -r file; do
    [[ -z "$file" ]] && continue
    
    if [[ "$file" =~ ^catalogs/ ]]; then
      needs_catalogs=true
    elif [[ "$file" =~ ^icons/ ]]; then
      needs_icons=true
    elif [[ "$file" =~ ^pkgsinfo/ ]]; then
      changed_pkgsinfo+="$file"$'\n'
    fi
  done <<< "$changed_files"
  
  local result=""
  [[ "$needs_catalogs" == "true" ]] && result+="catalogs "
  [[ "$needs_icons" == "true" ]] && result+="icons "
  [[ -n "$changed_pkgsinfo" ]] && result+="pkgsinfo"
  
  result=$(echo "$result" | xargs)
  
  if [[ -z "$result" ]]; then
    log "Changes detected but not in sync-relevant directories. Skipping sync."
    echo "none"
  else
    echo "$changed_pkgsinfo" > "$BASE/.git/logs/.changed-pkgsinfo-$$"
    echo "$result"
  fi
}

# ── targeted path download ─────────────────────────────────
if [[ ${#TARGET_PATHS[@]} -gt 0 ]]; then
  log ">> targeted path download - downloading ${#TARGET_PATHS[@]} specific package(s)"
  
  check_aws_auth
  
  for target_path in "${TARGET_PATHS[@]}"; do
    if [[ "$target_path" != pkgs/* ]]; then
      target_path="pkgs/$target_path"
    fi
    
    local_path="$BASE/$target_path"
    remote_path="$S3_URL/$target_path"
    
    mkdir -p "$(dirname "$local_path")"
    
    log "  -> syncing: $target_path"
    
    if [[ "$DRY_RUN" == "true" ]]; then
      log "  [DRY RUN] Would download: $remote_path -> $local_path"
    else
      aws_cmd s3 cp "$remote_path" "$local_path" 2>&1 | tee -a "$LOGFILE" || {
        log "  WARNING: Failed to download $target_path (may not exist in S3)"
      }
    fi
  done
  
  log 'Targeted path sync complete'
  mv "$LOGFILE" "${LOGFILE%.*}-$(next_index).log"
  exit 0
fi

# ── branch gate ────────────────────────────────────────────
current_branch=$(git symbolic-ref --quiet --short HEAD || true)
if [[ "$current_branch" != "main" ]]; then
  log "Skipping downloads – we are in branch '$current_branch', not main."
  mv "$LOGFILE" "${LOGFILE%.*}-$(next_index).log"
  exit 0
fi

log "On branch 'main' – checking for changes"

# ── change detection and selective sync ────────────────────
changed_paths=$(detect_changes)

if [[ "$changed_paths" == "none" ]]; then
  log "No sync needed"
  mv "$LOGFILE" "${LOGFILE%.*}-$(next_index).log"
  exit 0
fi

# ── AWS authentication ─────────────────────────────────────
check_aws_auth

# ── cleanup .DS_Store and stale temp files ────────────────
log "Cleaning up .DS_Store and stale temp files"
find "$BASE" -name ".DS_Store" -type f -delete 2>/dev/null || true
find "$BASE/.git/logs" -name ".changed-pkgsinfo-*" -type f -mmin +60 -delete 2>/dev/null || true

# ── handle force mode ──────────────────────────────────────
if [[ "$changed_paths" == "force" ]]; then
  log "WARNING: FORCE MODE - This will download ALL packages"
  echo "This operation will download the entire repository from S3."
  echo ""
  read -p "Continue with full download? (y/N) " -n 1 -r
  echo ""
  [[ ! $REPLY =~ ^[Yy]$ ]] && { log "Aborted."; exit 0; }
  
  log ">> force downloading catalogs"
  aws_cmd s3 sync "$S3_URL/catalogs/" "$BASE/catalogs" \
    --delete --exclude "*.DS_Store" 2>&1 | tee -a "$LOGFILE"
  
  log ">> force downloading pkgs"
  aws_cmd s3 sync "$S3_URL/pkgs/" "$BASE/pkgs" \
    --delete --exclude "*.DS_Store" 2>&1 | tee -a "$LOGFILE"
  
  log ">> force downloading icons"
  aws_cmd s3 sync "$S3_URL/icons/" "$BASE/icons" \
    --delete --exclude "*.DS_Store" 2>&1 | tee -a "$LOGFILE"
  
  log "Force download complete"
  mv "$LOGFILE" "${LOGFILE%.*}-$(next_index).log"
  exit 0
fi

# ── handle sync mode ───────────────────────────────────────
if [[ "$changed_paths" == "sync" ]]; then
  log ">> sync mode - full folder sync"
  
  SYNC_OPTS=(--delete --exclude "*.DS_Store")
  if [[ "$DRY_RUN" == "true" ]]; then
    SYNC_OPTS+=(--dryrun)
    log "DRY RUN MODE - No actual downloads will occur"
  fi
  
  log '>> syncing catalogs'
  aws_cmd s3 sync "$S3_URL/catalogs/" "$BASE/catalogs" "${SYNC_OPTS[@]}" 2>&1 | tee -a "$LOGFILE"
  
  log '>> syncing pkgs'
  aws_cmd s3 sync "$S3_URL/pkgs/" "$BASE/pkgs" "${SYNC_OPTS[@]}" 2>&1 | tee -a "$LOGFILE"
  
  log '>> syncing icons'
  aws_cmd s3 sync "$S3_URL/icons/" "$BASE/icons" "${SYNC_OPTS[@]}" 2>&1 | tee -a "$LOGFILE"
  
  log 'Sync mode complete'
  mv "$LOGFILE" "${LOGFILE%.*}-$(next_index).log"
  exit 0
fi

# ── default: targeted downloads based on changes ───────────

if echo "$changed_paths" | grep -q "catalogs"; then
  log '>> syncing catalogs'
  SYNC_OPTS=(--exclude "*.DS_Store")
  [[ "$DRY_RUN" == "true" ]] && SYNC_OPTS+=(--dryrun)
  aws_cmd s3 sync "$S3_URL/catalogs/" "$BASE/catalogs" "${SYNC_OPTS[@]}" 2>&1 | tee -a "$LOGFILE"
fi

if echo "$changed_paths" | grep -q "icons"; then
  log '>> syncing icons'
  SYNC_OPTS=(--exclude "*.DS_Store")
  [[ "$DRY_RUN" == "true" ]] && SYNC_OPTS+=(--dryrun)
  aws_cmd s3 sync "$S3_URL/icons/" "$BASE/icons" "${SYNC_OPTS[@]}" 2>&1 | tee -a "$LOGFILE"
fi

if echo "$changed_paths" | grep -q "pkgsinfo"; then
  pkgsinfo_cache="$BASE/.git/logs/.changed-pkgsinfo-$$"
  
  if [[ -f "$pkgsinfo_cache" ]]; then
    log '>> extracting package locations from changed pkgsinfo files'
    changed_pkgsinfo=$(cat "$pkgsinfo_cache")
    installer_locations=$(extract_installer_locations "$changed_pkgsinfo")
    
    if [[ -n "$installer_locations" ]]; then
      pkg_count=$(echo "$installer_locations" | wc -l | xargs)
      log ">> downloading $pkg_count specific package(s)"
      
      while IFS= read -r pkg_path; do
        [[ -z "$pkg_path" ]] && continue
        
        source_path="$S3_URL/pkgs/$pkg_path"
        dest_path="$BASE/pkgs/$pkg_path"
        dest_dir=$(dirname "$dest_path")
        
        mkdir -p "$dest_dir"
        
        log "  - $pkg_path"
        
        if [[ "$DRY_RUN" == "true" ]]; then
          log "    [DRY RUN] Would download: $source_path -> $dest_path"
        else
          aws_cmd s3 cp "$source_path" "$dest_path" 2>&1 | tee -a "$LOGFILE" || {
            log "    WARNING: Failed to download $pkg_path (may not exist remotely)"
          }
        fi
      done <<< "$installer_locations"
    else
      log "No installer_item_location entries found in changed pkgsinfo files"
    fi
    
    rm -f "$pkgsinfo_cache"
  fi
fi

# ── cleanup orphan packages ────────────────────────────────
log "Checking for orphan packages..."
cleanup_orphan_packages

log 'Git post-rewrite hook complete'
mv "$LOGFILE" "${LOGFILE%.*}-$(next_index).log"
