#!/usr/bin/env bash
#
# ──────────────────────────────────────────────────────────────────────────────
#  post-rewrite hook  –  targeted package downloads after rebase/amend
#
#  This hook fires after git operations that rewrite commits:
#       - `git rebase`
#       - `git commit --amend`
#       - `git filter-branch`
#
#  When commits are rewritten, package references in pkgsinfo may change.
#  This hook ensures your local pkgs directory stays in sync with Azure.
#
#  Default behavior (no flags):
#       - Parses changed pkgsinfo files (YAML/plist)
#       - Extracts installer_item_location from each
#       - Downloads ONLY those specific packages using hash-based azcopy copy
#       - Syncs catalogs/icons folders if changed
#
#  Flags:
#       --sync    : Full folder sync with hash comparison (incremental)
#       --force   : Full download with confirmation (forces re-download of all)
#       --dry-run : Show what would be downloaded without actually downloading
#       --path    : Download a specific package path
#
#  Configuration (set these environment variables or edit defaults below):
#       MUNKI_REPO_PATH       - Path to your Munki repo (default: ~/munki-repo)
#       MUNKI_STORAGE_ACCOUNT - Azure storage account name
#       MUNKI_CONTAINER       - Azure blob container name (default: munki)
#
#  Daily rotating log kept in .git/logs (7-day retention).
# ──────────────────────────────────────────────────────────────────────────────

set -euo pipefail
set -f
shopt -s nullglob

# ── Configuration ──────────────────────────────────────────
# These should be set via environment variables or edited here
BASE="${MUNKI_REPO_PATH:-$HOME/munki-repo}"
STORAGE_ACCOUNT="${MUNKI_STORAGE_ACCOUNT:-yourstorageaccount}"
CONTAINER="${MUNKI_CONTAINER:-munki}"
STORAGE_URL="https://${STORAGE_ACCOUNT}.blob.core.windows.net/${CONTAINER}"

# ── check for azcopy ───────────────────────────────────────
if [[ -x "/opt/homebrew/bin/azcopy" ]]; then
  AZCOPY="/opt/homebrew/bin/azcopy"
elif command -v azcopy &>/dev/null; then
  AZCOPY=$(command -v azcopy)
else
  echo "" >&2
  echo "ERROR: azcopy not found" >&2
  echo "" >&2
  echo "The post-rewrite hook needs azcopy to download packages from Azure Blob Storage." >&2
  echo "" >&2
  echo "To install azcopy:" >&2
  echo "  brew install azcopy" >&2
  echo "" >&2
  echo "Or download from: https://aka.ms/downloadazcopy" >&2
  echo "" >&2
  exit 1
fi

# ── command line options ───────────────────────────────────
SYNC_MODE=false
FORCE_MODE=false
DRY_RUN=false
TARGET_PATHS=()

while [[ $# -gt 0 ]]; do
  case $1 in
    --sync)
      SYNC_MODE=true
      shift
      ;;
    --force)
      FORCE_MODE=true
      shift
      ;;
    --dry-run)
      DRY_RUN=true
      shift
      ;;
    --path)
      shift
      [[ $# -gt 0 ]] && TARGET_PATHS+=("$1")
      shift
      ;;
    *)
      shift
      ;;
  esac
done

# ── logging ────────────────────────────────────────────────
LOGDIR="$BASE/.git/logs"
mkdir -p "$LOGDIR"
TODAY=$(date +%F)
LOGFILE="$LOGDIR/hook-post-rewrite-download-$TODAY.log"
: >"$LOGFILE"
find "$LOGDIR" -name 'hook-post-rewrite-download-*.log' -mtime +7 -exec rm {} \;

log() {
  local ts=$(date '+%Y-%m-%d %H:%M:%S')
  printf '%s %s\n' "$ts" "$*" | tee -a "$LOGFILE"
}

next_index() {
  local max=0
  for f in "$LOGDIR"/hook-post-rewrite-download-"$TODAY"-*; do
    [[ -e $f ]] || continue
    local n=${f##*-}; n=${n%.log}
    (( n > max )) && max=$n
  done
  echo $((max+1))
}

# ── Azure authentication check ─────────────────────────────
check_azure_auth() {
  local timeout_cmd=""
  if command -v gtimeout &>/dev/null; then
    timeout_cmd="gtimeout 10"
  else
    timeout_cmd="perl -e 'alarm shift @ARGV; exec @ARGV' 10"
  fi
  
  if ! eval "$timeout_cmd az account show &>/dev/null"; then
    log "Azure CLI not authenticated or timed out. Please run 'az login' first."
    exit 1
  fi
}

# ── parse pkgsinfo files for installer locations ───────────
extract_location_from_pkgsinfo() {
  local pkgsinfo_file="$1"
  local location=""
  
  if [[ "$pkgsinfo_file" =~ \.(yaml|yml)$ ]]; then
    location=$(grep -E '^\s*installer_item_location:\s*' "$pkgsinfo_file" 2>/dev/null | sed -E 's/^\s*installer_item_location:\s*//' | tr -d '"' | xargs | head -1)
  else
    location=$(plutil -extract installer_item_location raw "$pkgsinfo_file" 2>/dev/null | xargs || true)
  fi
  
  echo "$location"
}

extract_installer_locations() {
  local changed_pkgsinfo="$1"
  local installer_locations=()
  
  while IFS= read -r pkgsinfo_file; do
    [[ -z "$pkgsinfo_file" ]] && continue
    [[ ! -f "$BASE/$pkgsinfo_file" ]] && continue
    
    local location=$(extract_location_from_pkgsinfo "$BASE/$pkgsinfo_file")
    
    if [[ -n "$location" ]]; then
      installer_locations+=("$location")
      log "  Found package: $location"
    fi
  done <<< "$changed_pkgsinfo"
  
  printf '%s\n' "${installer_locations[@]}"
}

# ── build canonical package list from ALL pkgsinfo files ───
build_canonical_pkg_list() {
  local pkgsinfo_dir="$BASE/pkgsinfo"
  local canonical_list=()
  
  while IFS= read -r -d '' pkgsinfo_file; do
    local location=$(extract_location_from_pkgsinfo "$pkgsinfo_file")
    if [[ -n "$location" ]]; then
      canonical_list+=("$location")
    fi
  done < <(find "$pkgsinfo_dir" -type f \( -name "*.yaml" -o -name "*.yml" -o -name "*.plist" -o ! -name "*.*" \) -print0 2>/dev/null)
  
  printf '%s\n' "${canonical_list[@]}" | sort -u
}

# ── cleanup orphan packages not referenced by any pkgsinfo ─
cleanup_orphan_packages() {
  local pkgs_dir="$BASE/pkgs"
  local canonical_file=$(mktemp)
  local orphan_count=0
  
  log "Building canonical package list from pkgsinfo files..."
  build_canonical_pkg_list > "$canonical_file"
  local canonical_count=$(wc -l < "$canonical_file" | xargs)
  log "Found $canonical_count packages referenced in pkgsinfo"
  
  log "Scanning for orphan packages..."
  while IFS= read -r -d '' pkg_file; do
    local rel_path="${pkg_file#$pkgs_dir/}"
    
    if ! grep -qxF "$rel_path" "$canonical_file"; then
      log "  Orphan: $rel_path"
      if [[ "$DRY_RUN" == "true" ]]; then
        log "    [DRY RUN] Would delete"
      else
        rm -f "$pkg_file"
        log "    Deleted"
      fi
      ((orphan_count++))
    fi
  done < <(find "$pkgs_dir" -type f \( -name "*.pkg" -o -name "*.dmg" -o -name "*.zip" -o -name "*.mobileconfig" \) -print0 2>/dev/null)
  
  rm -f "$canonical_file"
  
  if [[ $orphan_count -gt 0 ]]; then
    log "Cleaned up $orphan_count orphan package(s)"
    find "$pkgs_dir" -type d -empty -delete 2>/dev/null || true
  else
    log "No orphan packages found"
  fi
}

# ── change detection ───────────────────────────────────────
detect_changes() {
  if [[ "$FORCE_MODE" == "true" ]]; then
    echo "force"
    return
  fi
  
  if [[ "$SYNC_MODE" == "true" ]]; then
    echo "sync"
    return
  fi
  
  if ! git rev-parse HEAD~1 >/dev/null 2>&1; then
    log "No previous commit available for change detection. Use --sync for full sync."
    echo "none"
    return
  fi
  
  local changed_files=$(git diff --name-only HEAD~1 HEAD -- pkgsinfo/ catalogs/ icons/ 2>/dev/null || true)
  
  if [[ -z "$changed_files" ]]; then
    log "No changes detected in tracked directories. Skipping sync."
    echo "none"
    return
  fi
  
  local needs_catalogs=false
  local needs_icons=false
  local changed_pkgsinfo=""
  
  while IFS= read -r file; do
    [[ -z "$file" ]] && continue
    
    if [[ "$file" =~ ^catalogs/ ]]; then
      needs_catalogs=true
    elif [[ "$file" =~ ^icons/ ]]; then
      needs_icons=true
    elif [[ "$file" =~ ^pkgsinfo/ ]]; then
      changed_pkgsinfo+="$file"$'\n'
    fi
  done <<< "$changed_files"
  
  local result=""
  [[ "$needs_catalogs" == "true" ]] && result+="catalogs "
  [[ "$needs_icons" == "true" ]] && result+="icons "
  [[ -n "$changed_pkgsinfo" ]] && result+="pkgsinfo"
  
  result=$(echo "$result" | xargs)
  
  if [[ -z "$result" ]]; then
    log "Changes detected but not in sync-relevant directories. Skipping sync."
    echo "none"
  else
    echo "$changed_pkgsinfo" > "$BASE/.git/logs/.changed-pkgsinfo-$$"
    echo "$result"
  fi
}

# ── targeted path download ─────────────────────────────────
if [[ ${#TARGET_PATHS[@]} -gt 0 ]]; then
  log ">> targeted path download - downloading ${#TARGET_PATHS[@]} specific package(s)"
  
  check_azure_auth
  export AZCOPY_AUTO_LOGIN_TYPE="AZCLI"
  
  for target_path in "${TARGET_PATHS[@]}"; do
    if [[ "$target_path" != pkgs/* ]]; then
      target_path="pkgs/$target_path"
    fi
    
    local_path="$BASE/$target_path"
    remote_url="$STORAGE_URL/$target_path"
    
    mkdir -p "$(dirname "$local_path")"
    
    log "  -> syncing: $target_path"
    
    if [[ "$DRY_RUN" == "true" ]]; then
      log "  [DRY RUN] Would download: $remote_url -> $local_path"
    else
      "$AZCOPY" copy "$remote_url" "$local_path" \
        --log-level="ERROR" --output-level="essential" 2>&1 | tee -a "$LOGFILE" || {
          log "  WARNING: Failed to download $target_path (may not exist in Azure)"
        }
    fi
  done
  
  log 'Targeted path sync complete'
  mv "$LOGFILE" "${LOGFILE%.*}-$(next_index).log"
  exit 0
fi

# ── branch gate ────────────────────────────────────────────
current_branch=$(git symbolic-ref --quiet --short HEAD || true)
if [[ "$current_branch" != "main" ]]; then
  log "Skipping downloads – we are in branch '$current_branch', not main."
  mv "$LOGFILE" "${LOGFILE%.*}-$(next_index).log"
  exit 0
fi

log "On branch 'main' – checking for changes"

# ── change detection and selective sync ────────────────────
changed_paths=$(detect_changes)

if [[ "$changed_paths" == "none" ]]; then
  log "No sync needed"
  mv "$LOGFILE" "${LOGFILE%.*}-$(next_index).log"
  exit 0
fi

# ── Azure authentication ───────────────────────────────────
check_azure_auth

# ── cleanup .DS_Store and stale temp files ────────────────
log "Cleaning up .DS_Store and stale temp files"
find "$BASE" -name ".DS_Store" -type f -delete 2>/dev/null || true
find "$BASE" -name ".az*" -type f -delete 2>/dev/null || true
find "$BASE" -name "*.gzPk4O" -type f -delete 2>/dev/null || true
find "$BASE/.git/logs" -name ".changed-pkgsinfo-*" -type f -mmin +60 -delete 2>/dev/null || true

# ── set azcopy auth ────────────────────────────────────────
export AZCOPY_AUTO_LOGIN_TYPE="AZCLI"

# ── handle force mode ──────────────────────────────────────
if [[ "$changed_paths" == "force" ]]; then
  log "WARNING: FORCE MODE - This will download ALL packages"
  echo "This operation will download the entire repository from Azure Blob Storage."
  echo ""
  read -p "Continue with full download? (y/N) " -n 1 -r
  echo ""
  [[ ! $REPLY =~ ^[Yy]$ ]] && { log "Aborted."; exit 0; }
  
  log ">> force downloading catalogs"
  "$AZCOPY" sync "$STORAGE_URL/catalogs/" "$BASE/catalogs" \
    --delete-destination=true --exclude-pattern="*.DS_Store" --log-level="INFO" 2>&1 | tee -a "$LOGFILE"
  
  log ">> force downloading pkgs"
  "$AZCOPY" sync "$STORAGE_URL/pkgs/" "$BASE/pkgs" \
    --delete-destination=true --exclude-pattern="*.DS_Store" --log-level="INFO" 2>&1 | tee -a "$LOGFILE"
  
  log ">> force downloading icons"
  "$AZCOPY" sync "$STORAGE_URL/icons/" "$BASE/icons" \
    --delete-destination=true --exclude-pattern="*.DS_Store" --log-level="INFO" 2>&1 | tee -a "$LOGFILE"
  
  log "Force download complete"
  mv "$LOGFILE" "${LOGFILE%.*}-$(next_index).log"
  exit 0
fi

# ── handle sync mode ───────────────────────────────────────
if [[ "$changed_paths" == "sync" ]]; then
  log ">> sync mode - full folder sync with hash comparison"
  
  if [[ "$DRY_RUN" == "true" ]]; then
    SYNC_OPTS=(--delete-destination=true --exclude-pattern="*.DS_Store" --dry-run --log-level="INFO")
    log "DRY RUN MODE - No actual downloads will occur"
  else
    SYNC_OPTS=(--delete-destination=true --exclude-pattern="*.DS_Store" --log-level="ERROR" --output-level="essential")
  fi
  
  log '>> syncing catalogs'
  "$AZCOPY" sync "$STORAGE_URL/catalogs/" "$BASE/catalogs" "${SYNC_OPTS[@]}" 2>&1 | \
    grep -E "(DRYRUN|Job.*completed|Number of Copy Transfers|Elapsed Time|Final Job Status)" | tee -a "$LOGFILE"
  
  log '>> syncing pkgs'
  "$AZCOPY" sync "$STORAGE_URL/pkgs/" "$BASE/pkgs" "${SYNC_OPTS[@]}" 2>&1 | \
    grep -E "(DRYRUN|Job.*completed|Number of Copy Transfers|Elapsed Time|Final Job Status)" | tee -a "$LOGFILE"
  
  log '>> syncing icons'
  "$AZCOPY" sync "$STORAGE_URL/icons/" "$BASE/icons" "${SYNC_OPTS[@]}" 2>&1 | \
    grep -E "(DRYRUN|Job.*completed|Number of Copy Transfers|Elapsed Time|Final Job Status)" | tee -a "$LOGFILE"
  
  log 'Sync mode complete'
  mv "$LOGFILE" "${LOGFILE%.*}-$(next_index).log"
  exit 0
fi

# ── default: targeted downloads based on changes ───────────

if echo "$changed_paths" | grep -q "catalogs"; then
  log '>> syncing catalogs'
  "$AZCOPY" sync "$STORAGE_URL/catalogs/" "$BASE/catalogs" \
    --exclude-pattern="*.DS_Store" --log-level="ERROR" --output-level="essential" 2>&1 | tee -a "$LOGFILE"
fi

if echo "$changed_paths" | grep -q "icons"; then
  log '>> syncing icons'
  "$AZCOPY" sync "$STORAGE_URL/icons/" "$BASE/icons" \
    --exclude-pattern="*.DS_Store" --log-level="ERROR" --output-level="essential" 2>&1 | tee -a "$LOGFILE"
fi

if echo "$changed_paths" | grep -q "pkgsinfo"; then
  pkgsinfo_cache="$BASE/.git/logs/.changed-pkgsinfo-$$"
  
  if [[ -f "$pkgsinfo_cache" ]]; then
    log '>> extracting package locations from changed pkgsinfo files'
    changed_pkgsinfo=$(cat "$pkgsinfo_cache")
    installer_locations=$(extract_installer_locations "$changed_pkgsinfo")
    
    if [[ -n "$installer_locations" ]]; then
      pkg_count=$(echo "$installer_locations" | wc -l | xargs)
      log ">> downloading $pkg_count specific package(s)"
      
      while IFS= read -r pkg_path; do
        [[ -z "$pkg_path" ]] && continue
        
        source_url="$STORAGE_URL/pkgs/$pkg_path"
        dest_path="$BASE/pkgs/$pkg_path"
        dest_dir=$(dirname "$dest_path")
        
        mkdir -p "$dest_dir"
        
        log "  - $pkg_path"
        
        "$AZCOPY" copy "$source_url" "$dest_path" \
          --overwrite=ifSourceNewer --check-md5=FailIfDifferent \
          --log-level="ERROR" --output-level="essential" 2>&1 | tee -a "$LOGFILE" || {
          log "    WARNING: Failed to download $pkg_path (may not exist remotely)"
        }
      done <<< "$installer_locations"
    else
      log "No installer_item_location entries found in changed pkgsinfo files"
    fi
    
    rm -f "$pkgsinfo_cache"
  fi
fi

# ── cleanup orphan packages ────────────────────────────────
log "Checking for orphan packages..."
cleanup_orphan_packages

log 'Git post-rewrite hook complete'
mv "$LOGFILE" "${LOGFILE%.*}-$(next_index).log"
